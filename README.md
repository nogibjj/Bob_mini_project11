# Mini Project 11: Data Pipeline with Databricks
A data pipeline is designed to carry out the necessary procedures for transferring data from its original sources, modifying it according to specific needs, and then storing it in a designated system. This pipeline encompasses every step needed to convert unprocessed data into a format that is ready for use. In practice, this means preparing the data in such a way that data analysts and scientists can derive insights from it by conducting various analyses and generating reports. ETL workflow is a typical example of data pipeline. In the ETL (Extract, Transform, Load) process, data is initially extracted from various source systems and then loaded into a staging area. Within this staging area, the data undergoes a series of transformations to meet specific criteria, which includes enhancing data quality, removing duplicate records, among other modifications. After these transformations, the data is then loaded into a final destination, typically a data warehouse or a data lake, for storage and further use.

# Create Pipeline 
<img width="1521" alt="Screenshot 2023-11-12 at 23 42 44" src="https://github.com/nogibjj/Bob_mini_project11/assets/141781876/9a01f49e-3b34-48ed-9bd4-675e524ee927">
