# Mini Project 11: Data Pipeline with Databricks
A data pipeline is designed to carry out the necessary procedures for transferring data from its original sources, modifying it according to specific needs, and then storing it in a designated system. This pipeline encompasses every step needed to convert unprocessed data into a format that is ready for use. In practice, this means preparing the data in such a way that data analysts and scientists can derive insights from it by conducting various analyses and generating reports. ETL workflow is a typical example of data pipeline. In the ETL (Extract, Transform, Load) process, data is initially extracted from various source systems and then loaded into a staging area. Within this staging area, the data undergoes a series of transformations to meet specific criteria, which includes enhancing data quality, removing duplicate records, among other modifications. After these transformations, the data is then loaded into a final destination, typically a data warehouse or a data lake, for storage and further use.

# Create Pipeline 
### Extracting data
<img width="1478" alt="Screenshot 2023-11-12 at 23 54 33" src="https://github.com/nogibjj/Bob_mini_project11/assets/141781876/a87a92d0-bf8d-427a-91de-9b1d405cace4">

### Preparing data
Subset the extracted data into a new table
<img width="1474" alt="Screenshot 2023-11-12 at 23 54 49" src="https://github.com/nogibjj/Bob_mini_project11/assets/141781876/b12f372e-14ca-47bd-97bb-6a172c4231f1">

### Analysis
<img width="1483" alt="Screenshot 2023-11-12 at 23 55 05" src="https://github.com/nogibjj/Bob_mini_project11/assets/141781876/219af064-c97d-4c6f-b20d-3ca6c130ae62">

<img width="1070" alt="Screenshot 2023-11-12 at 23 55 28" src="https://github.com/nogibjj/Bob_mini_project11/assets/141781876/bf2c055d-8af1-4668-aaf3-11b52f0809a6">

### Workflow overview
<img width="1521" alt="Screenshot 2023-11-12 at 23 42 44" src="https://github.com/nogibjj/Bob_mini_project11/assets/141781876/9a01f49e-3b34-48ed-9bd4-675e524ee927">
